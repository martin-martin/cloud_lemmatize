{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the pipeline!\n",
    "Final sprint for language one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting files and setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the language being processed (has implication on file location and output!)\n",
    "language = \"es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "start = time.time()\n",
    "FORMAT = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(format=FORMAT, filename='{0}.log'.format(language), level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = \"chunk_out\"\n",
    "logging.info(\"====================BEGIN PROCESSING {} FILES====================\".format(language))\n",
    "all_files = os.listdir(\"{0}/{1}\".format(folder, language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch everything before the first occurance of a opening curly brace\n",
    "# to remove HTTP response headers present in the files\n",
    "pattern = re.compile(r\"[^{]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up the data files for JSON traversion\n",
    "documentation = \"\"\n",
    "response_list = list()\n",
    "\n",
    "for a_file in all_files:\n",
    "    # fetch the current file\n",
    "    with open(\"{0}/{1}/{2}\".format(folder, language, a_file)) as f:\n",
    "        file_string = f.read()\n",
    "        header = re.search(pattern, file_string)\n",
    "        documentation += \"\\t{0}\\n\\n{1}\".format(a_file, header.group(0))\n",
    "        # delete the HTML response header from the JSON\n",
    "        file_string = file_string.replace(header.group(0), \"\")\n",
    "        # load the JSON into a python dict\n",
    "        response = json.loads(file_string)\n",
    "        response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the documentation headers, just because\n",
    "with open(\"{0}doc_headers.txt\".format(language), \"w\") as f:\n",
    "    f.write(documentation)\n",
    "logging.info(\"*FINISHED WRITING HEADER DOCUMENTATION FILE*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def item_generator(json_input, lookup_key):\n",
    "    \"\"\"traverses the JSON structure to find entries with specific key names.\"\"\"\n",
    "    if isinstance(json_input, dict):\n",
    "        for k, v in json_input.items():\n",
    "            if k == lookup_key:\n",
    "                yield v\n",
    "            else:\n",
    "                for child_val in item_generator(v, lookup_key):\n",
    "                    yield child_val\n",
    "    elif isinstance(json_input, list):\n",
    "        for item in json_input:\n",
    "            for item_val in item_generator(item, lookup_key):\n",
    "                yield item_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#response_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "focus_list = list()\n",
    "#messy_list = list()\n",
    "\n",
    "for i, response in enumerate(response_list):\n",
    "    now = time.time()\n",
    "    logging.info(\"processing file {0}\".format(i))\n",
    "    # '_[0]' takes the first entry of an 'analysis_list' object\n",
    "    # later entries are regarding different potential semantical meanings and might be interesting\n",
    "    # for another time and type of analysis\n",
    "    # the first entry holds all the information we currently need\n",
    "    for _ in item_generator(response, \"analysis_list\"):\n",
    "        # ATTENTION:\n",
    "        # meaningcloud also analyzes 'phrases', grouping specific words together\n",
    "        # for now I take only those forward that are actually single words\n",
    "        # the 'phrases' are only groups of single words, meaning that every word\n",
    "        # that appears in a 'phrase' does also appear as a single word\n",
    "        # since that's the unit we're dealing with, I only collect the single words\n",
    "        #\n",
    "        # if original form consists of more than one word there is a space present\n",
    "        # therefore searching for \" \" would not return '-1' (= 'not found')\n",
    "        if _[0][\"original_form\"].find(\" \") == -1:\n",
    "            focus_list.append(_[0])\n",
    "        # put the rest into another list (for checkup)\n",
    "        #else:\n",
    "            #messy_list.append(_[0])\n",
    "    logging.info(\"finished processing file {0} in {1} magical time units\".format(i, time.time()-now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propping the data into a nice format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(focus_list)\n",
    "\n",
    "# keeping only the the columns we need now\n",
    "fdf = df[['lemma', 'original_form', 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exporting to a csv file\n",
    "fdf.to_csv(\"lemmatizationTagged/{}_lemPOS.csv\".format(language), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"====================FINISHED====================\")\n",
    "logging.info(\"Processed {0} files, took {1} special time units\".format(len(all_files), time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:car]",
   "language": "python",
   "name": "conda-env-car-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
